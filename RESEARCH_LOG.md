# Phase 2: Enabling Strategic Card Selection & Automation

**Date:** September 26, 2025

### 1. Methodology

Following the establishment of a baseline with the "blind" agent, Phase 2 focused on enabling the agent's ability to strategically select which cards to play from its hand. This involved two key changes to the project's methodology:

* **Activation of the `rank_selection` Head:** The fourth head of the Deep Q-Network, responsible for outputting Q-values for each card rank in the agent's hand, was fully implemented and enabled. The loss function in `rl_agent.py` was updated to perform backpropagation on the Q-values corresponding to the ranks of the cards used in each "play" action.
* **Automated Experimentation Framework:** Recognizing the significant increase in training time and the high variance between runs, an automated experimentation script, `run_experiments.py`, was developed. This script orchestrates multiple, independent training sessions by invoking `main.py` with a unique output directory for each run. This approach ensures that logs and model checkpoints from each experiment are stored separately, allowing for robust, parallelized data collection and a more accurate assessment of the agent's average performance. To accommodate this, `main.py` was refactored to accept an `--output_dir` command-line argument.

### 2. Observation: High Variance and Learning Failure

With the new architecture in place, an initial training run over 50k episodes with a medium exploration decay (`0.999985`) yielded a promising peak win rate of **35%**, significantly outperforming the blind agent's 25.8% baseline.

However, subsequent attempts to replicate this success under more extensive training regimes (100k episodes with a longer exploration decay of `0.999995`) resulted in catastrophic learning failure. Out of nine independent runs, seven converged to a stable win rate of 0%. The agent's policy would collapse into a suboptimal state during the prolonged exploration phase and never recover.

This demonstrates a critical issue of high variance and poor sample efficiency. The initial success appears to have been a statistical outlier (a "golden run"), while the systematic failures indicate a fundamental problem in the learning dynamics.

### 3. Hypothesis: The Sparse Reward Problem

The introduction of the fourth network head exponentially increased the complexity of the action space and, consequently, the difficulty of the credit assignment problem. The agent's learning signal consists solely of a terminal reward (+1.0 or -1.0) at the end of an episode.

Our hypothesis is that this **sparse reward signal** is insufficient to guide the agent through the vast, low-quality action space during exploration. When epsilon is high, the agent fills its replay buffer with millions of transitions generated by terrible random actions, all of which are associated with a delayed, final reward of -1.0. The network learns that virtually all actions lead to a loss, causing the Q-values to converge to similar negative values and preventing the emergence of a coherent policy.

### 4. Proposed Solution: Reward Shaping

To combat the sparse reward problem, we will introduce 2 rewards that will give the agent a feedback if it was a good or a bad result. This technique, known as **Reward Shaping**, provides a denser, more immediate feedback signal to guide the agent's learning process.

The following reward structure will be implemented in `environment.py`:

* **+0.05** when the agent makes someone buy cards.
* **-0.05** when the agent buy cards from the pile.

The terminal rewards of +1.0 for a win and -1.0 for a loss will remain, ensuring the primary objective is still optimized.

### 5. Expected Outcome

By providing more frequent feedback, the agent should be able to more effectively solve the credit assignment problem. It will learn the value of micro-actions (like making good challenges) even if it doesn't win the episode. This is expected to stabilize the training process, reduce the variance between runs, and allow the agent to consistently learn effective policies, even with prolonged exploration phases.